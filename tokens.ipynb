{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Tokenization with OpenAI\n",
    "\n",
    "This is some analysis using cl100k_base (the OpenAI embedding encoding). This byte-pair encoding is efficient for English text because it was optimized on the web dataset which includes large volumes of written English. \n",
    "This notebook is some analysis to establish a general rule of how many tokens-per-word to expect for Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kana\n",
    "All kana are either 1 or 2 tokens, depending on how commonly they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あ, い, う, え, お, か, が, き, く, け, こ, ご, さ, ざ, し, じ, す, せ, そ, た, だ, ち, っ, つ, て, で, と, ど, な, に, の, は, ば, ま, み, め, も, や, よ, ら, り, る, れ, ろ, わ, を, ん, ア, ィ, イ, ウ, ェ, エ, オ, カ, キ, ク, グ, コ, サ, シ, ジ, ス, ズ, セ, タ, ダ, チ, ッ, テ, デ, ト, ド, ナ, ニ, バ, パ, ビ, ピ, フ, ブ, プ, ペ, ポ, マ, ム, メ, ャ, ュ, ョ, ラ, リ, ル, レ, ロ, ン, ・, ー\n",
      "ぁ, ぃ, ぅ, ぇ, ぉ, ぎ, ぐ, げ, ず, ぜ, ぞ, ぢ, づ, ぬ, ね, ぱ, ひ, び, ぴ, ふ, ぶ, ぷ, へ, べ, ぺ, ほ, ぼ, ぽ, む, ゃ, ゅ, ゆ, ょ, ゎ, ゐ, ゑ, ゔ, ゕ, ゠, ァ, ゥ, ォ, ガ, ギ, ケ, ゲ, ゴ, ザ, ゼ, ソ, ゾ, ヂ, ツ, ヅ, ヌ, ネ, ノ, ハ, ヒ, ヘ, ベ, ホ, ボ, ミ, モ, ヤ, ユ, ヨ, ヮ, ワ, ヰ, ヱ, ヲ, ヴ, ヵ, ヶ, ヷ, ヸ, ヹ, ヺ, ヽ, ヾ, ｧ, ｨ, ｩ, ｪ, ｫ, ｬ, ｭ, ｮ, ｯ, ｰ, ｱ, ｲ, ｳ, ｴ, ｵ, ｶ, ｷ, ｸ, ｹ, ｺ, ｻ, ｼ, ｽ, ｾ, ｿ, ﾀ, ﾁ, ﾂ, ﾃ, ﾄ, ﾅ, ﾆ, ﾇ, ﾈ, ﾉ, ﾊ, ﾋ, ﾌ, ﾍ, ﾎ, ﾏ, ﾐ, ﾑ, ﾒ, ﾓ, ﾔ, ﾕ, ﾖ, ﾗ, ﾘ, ﾙ, ﾚ, ﾛ, ﾜ, ﾝ\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def chrs(start, end):\n",
    "    return [chr(i) for i in range(start, end)]\n",
    "\n",
    "punctuation = chrs(0x3000, 0x303f) + chrs(0x3099, 0x309f) # Not used.\n",
    "hiragana = chrs(0x3041, 0x3096)\n",
    "katakana = chrs(0x30a0, 0x30ff)\n",
    "half_width_katakana = chrs(0xff67, 0xff9e)\n",
    "\n",
    "all_kana = hiragana + katakana + half_width_katakana\n",
    "lengths = {}\n",
    "token_distributions = defaultdict(list)\n",
    "for kana in all_kana:\n",
    "    length = len(enc.encode(kana))\n",
    "    lengths[kana] = length\n",
    "    token_distributions[length].append(kana)\n",
    "\n",
    "\n",
    "print(', '.join(token_distributions[1]))\n",
    "print(', '.join(token_distributions[2]))\n",
    "assert token_distributions[3] == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ちょ 3 2\n",
      "っつ 2 2\n",
      "びゃ 4 2\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    \"ちょ\",\n",
    "    \"っつ\",\n",
    "    \"びゃ\",\n",
    "]\n",
    "for pair in pairs:\n",
    "    print(pair, len(enc.encode(pair)), len(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common sentence endings\n",
    "Some common endings to sentences are a single token, including past-tense verbs (mashita). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ます 1 2\n",
      "です 1 2\n",
      "ました 1 3\n",
      "でした 2 3\n",
      "ない 2 2\n"
     ]
    }
   ],
   "source": [
    "# Common pairs of full-width hiragana\n",
    "pairs = (\n",
    "    \"ます\",\n",
    "    \"です\",\n",
    "    \"ました\",\n",
    "    \"でした\",\n",
    "    \"ない\"\n",
    ")\n",
    "for pair in pairs:\n",
    "    print(pair, len(enc.encode(pair)), len(pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particles\n",
    "All common particles are 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "は 1\n",
      "に 1\n",
      "で 1\n",
      "が 1\n",
      "な 1\n"
     ]
    }
   ],
   "source": [
    "particles = \"はにでがな\"\n",
    "for particle in particles:\n",
    "    print(particle, len(enc.encode(particle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideographs and Kanji\n",
    "\n",
    "Kanji are 1,2 or 3 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n",
      "12516\n",
      "7983\n"
     ]
    }
   ],
   "source": [
    "ideographs = [chr(i) for i in range(0x4e00, 0x9faf)]\n",
    "\n",
    "ideograph_token_distributions = defaultdict(list)\n",
    "for kanji in ideographs:\n",
    "    length = len(enc.encode(kanji))\n",
    "    token_distributions[length].append(kanji)\n",
    "\n",
    "\n",
    "print(len(token_distributions[1]))\n",
    "print(len(token_distributions[2]))\n",
    "print(len(token_distributions[3]))\n",
    "assert token_distributions[4] == []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
