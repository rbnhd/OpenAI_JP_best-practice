{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets pandas azure-identity \"azure-search-documents==11.6.0b1\" azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyshaw/projects/azure-openai-japanese-best-practice/.venv/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for miracl/miracl-corpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/miracl/miracl-corpus\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "lang='ja'  # or any of the 16 languages\n",
    "miracl_corpus = datasets.load_dataset('miracl/miracl-corpus', lang, cache_dir='.cache')['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the documents in the corpus to Cosmos DB in batches (50 at a time)\n",
    "# This will take hours..\n",
    "from azure.cosmos.aio import CosmosClient\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# Replace the connection string with your own.\n",
    "client = CosmosClient(os.environ[\"AZURE_COSMOS_URI\"], credential=os.environ[\"AZURE_COSMOS_KEY\"])\n",
    "\n",
    "db = await client.create_database_if_not_exists(id='miracl')\n",
    "# setup container for this sample\n",
    "container = await db.create_container_if_not_exists(id='corpus',\n",
    "                                             partition_key=PartitionKey(path='/docid', kind='Hash'))\n",
    "\n",
    "for i in range(0, 40): # ~270,000 documents\n",
    "   test_corpus = miracl_corpus.shard(1000, i)\n",
    "\n",
    "   for batch in test_corpus.to_pandas(batched=True, batch_size=50):\n",
    "      documents = batch.to_dict(orient='records')\n",
    "      tasks = []\n",
    "      for doc in documents:\n",
    "         doc['id'] = doc['docid'].replace(\"#\", \"i\")\n",
    "         tasks.append(container.upsert_item(doc))\n",
    "      await asyncio.gather(*tasks)\n",
    "      print(f\"Added batch of 50 documents to CosmosDB in batch {i}, last index - {doc['docid']}.\")\n",
    "   print(f\"Added batch {i} to CosmosDB index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Capture a list of all the documents we did upload with the same sharding criteria as above \"\"\"\n",
    "indexed_docs = []\n",
    "for i in range(0, 40):\n",
    "   test_corpus = miracl_corpus.shard(1000, i)\n",
    "   for batch in test_corpus.to_pandas(batched=True, batch_size=50):\n",
    "      indexed_docs.extend([doc['docid'] for doc in batch.to_dict(orient='records')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyshaw/projects/azure-openai-japanese-best-practice/.venv/lib/python3.11/site-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyshaw/projects/azure-openai-japanese-best-practice/.venv/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for miracl/miracl contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/miracl/miracl\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import json\n",
    "\n",
    "token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "lang='ja'  # or any of the 16 languages\n",
    "miracl = datasets.load_dataset('miracl/miracl', lang, use_auth_token=token, cache_dir='.cache')\n",
    "\n",
    "# training set:\n",
    "questions = {}\n",
    "\n",
    "for data in miracl['train']:  # or 'dev', 'testA'\n",
    "  query_id = data['query_id']\n",
    "  query = data['query']\n",
    "  if data['query'] not in questions:\n",
    "    questions[data['query']] = {'positive_passages': [], 'negative_passages': []}\n",
    "\n",
    "  positive_passages = data['positive_passages']\n",
    "  negative_passages = data['negative_passages']\n",
    "  for entry in positive_passages:\n",
    "    if entry['docid'] in indexed_docs:\n",
    "      questions[data['query']]['positive_passages'].append(entry['docid'])\n",
    "  for entry in negative_passages:\n",
    "    if entry['docid'] in indexed_docs:\n",
    "      questions[data['query']]['negative_passages'].append(entry['docid'])\n",
    "\n",
    "with open('miracl_questions.json', 'w') as f:\n",
    "  json.dump(questions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3477"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(miracl['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._generated.models._models_py3.SearchIndexer at 0x11103d0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.search.documents.indexes.aio import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchFieldDataType,\n",
    ")\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "import os\n",
    "ANALYSER = \"ja.microsoft\"\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "index_name = 'test-miracl-index-ja-microsoft'\n",
    "indexer_name = 'test-miracl-indexer-ja-microsoft'\n",
    "azure_cred = DefaultAzureCredential()\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, azure_cred)\n",
    "index_client = SearchIndexClient(service_endpoint, azure_cred)\n",
    "indexer_client = SearchIndexerClient(service_endpoint, azure_cred)\n",
    "\n",
    "async def create_miracl_corpus_index(name):\n",
    "    try:\n",
    "        if await index_client.get_index(name):\n",
    "            return\n",
    "    except ResourceNotFoundError:\n",
    "        pass\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SimpleField(name=\"docid\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"text\", type=SearchFieldDataType.String, analyzer_name=ANALYSER),\n",
    "    ]\n",
    "    index = SearchIndex(\n",
    "        name=name,\n",
    "        fields=fields)\n",
    "    result = await index_client.create_index(index)\n",
    "    return result\n",
    "\n",
    "async def create_indexer(name, index_name):\n",
    "    try:\n",
    "        if await indexer_client.get_indexer(name):\n",
    "            return\n",
    "    except ResourceNotFoundError:\n",
    "        pass\n",
    "    # Create an indexer\n",
    "    indexer_name = f\"{name}-indexer\"\n",
    "\n",
    "    indexer = SearchIndexer(\n",
    "        name=indexer_name,\n",
    "        description=\"Indexer to index documents and generate embeddings\",\n",
    "        # skillset_name=f\"{name}-skillset\",\n",
    "        target_index_name=index_name,\n",
    "        data_source_name=f\"miracl-cosmos\",\n",
    "    )\n",
    "\n",
    "    return await indexer_client.create_or_update_indexer(indexer)\n",
    "\n",
    "await create_miracl_corpus_index(index_name)\n",
    "await create_indexer(indexer_name, index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
