{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIRACL Benchmark with analyzers\n",
    "\n",
    "This experiment is to compare the performance of the Microsoft and Lucene analyzers with the MIRACL benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets pandas azure-identity \"azure-search-documents==11.6.0b1\" azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the MIRACL Corpus (this a dump of a bunch of data from Japanese Wikipedia)\n",
    "import datasets\n",
    "\n",
    "lang='ja'  # or any of the 16 languages\n",
    "miracl_corpus = datasets.load_dataset('miracl/miracl-corpus', lang, cache_dir='.cache', trust_remote_code=True)['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the documents in the corpus to Cosmos DB in batches (50 at a time)\n",
    "# This will take hours..\n",
    "from azure.cosmos.aio import CosmosClient\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# Replace the connection string with your own.\n",
    "client = CosmosClient(os.environ[\"AZURE_COSMOS_URI\"], credential=os.environ[\"AZURE_COSMOS_KEY\"])\n",
    "\n",
    "db = await client.create_database_if_not_exists(id='miracl')\n",
    "# setup container for this sample\n",
    "container = await db.create_container_if_not_exists(id='corpus',\n",
    "                                             partition_key=PartitionKey(path='/docid', kind='Hash'))\n",
    "\n",
    "for i in range(0, 40): # ~270,000 documents\n",
    "   test_corpus = miracl_corpus.shard(1000, i)\n",
    "\n",
    "   for batch in test_corpus.to_pandas(batched=True, batch_size=50):\n",
    "      documents = batch.to_dict(orient='records')\n",
    "      tasks = []\n",
    "      for doc in documents:\n",
    "         doc['id'] = doc['docid'].replace(\"#\", \"i\")\n",
    "         tasks.append(container.upsert_item(doc))\n",
    "      await asyncio.gather(*tasks)\n",
    "      print(f\"Added batch of 50 documents to CosmosDB in batch {i}, last index - {doc['docid']}.\")\n",
    "   print(f\"Added batch {i} to CosmosDB index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Capture a list of all the documents we did upload with the same sharding criteria as above \"\"\"\n",
    "indexed_docs = []\n",
    "for i in range(0, 40):\n",
    "   test_corpus = miracl_corpus.shard(1000, i)\n",
    "   for batch in test_corpus.to_pandas(batched=True, batch_size=50):\n",
    "      indexed_docs.extend([doc['docid'] for doc in batch.to_dict(orient='records')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "import json\n",
    "\n",
    "token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "lang='ja'  # or any of the 16 languages\n",
    "miracl = datasets.load_dataset('miracl/miracl', lang, use_auth_token=token, cache_dir='.cache')\n",
    "\n",
    "# training set:\n",
    "questions = {}\n",
    "\n",
    "for data in miracl['train']:  # or 'dev', 'testA'\n",
    "  query_id = data['query_id']\n",
    "  query = data['query']\n",
    "  if data['query_id'] not in questions:\n",
    "    questions[data['query_id']] = {'query': data['query'], 'positive_passages': [], 'negative_passages': []}\n",
    "\n",
    "  positive_passages = data['positive_passages']\n",
    "  negative_passages = data['negative_passages']\n",
    "  for entry in positive_passages:\n",
    "    if entry['docid'] in indexed_docs:\n",
    "      questions[data['query_id']]['positive_passages'].append(entry['docid'])\n",
    "  for entry in negative_passages:\n",
    "    if entry['docid'] in indexed_docs:\n",
    "      questions[data['query_id']]['negative_passages'].append(entry['docid'])\n",
    "\n",
    "# Clean up the questions and remove any that don't have positive passages\n",
    "searchable_questions = {}\n",
    "for query_id, question in questions.items():\n",
    "  if len(question['positive_passages']) > 0:\n",
    "    searchable_questions[query_id] = question\n",
    "\n",
    "with open('data/miracl_questions.json', 'w') as f:\n",
    "  json.dump(searchable_questions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(miracl['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Automate the following:\n",
    "# - Create a datasource in Azure Search for Cosmos DB with data change tracking\n",
    "# - Create three indexes in Azure Search for the datasource with the right fields. One for each analyzer option\n",
    "# - Create a skillset in Azure Search with vectorizers\n",
    "# - Create an indexer in Azure Search to index the Cosmos DB data into the index\n",
    "# - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 海底ケーブルが初めて結ばれたのはどこ？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D2827F290>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D282FDB90>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D27E47890>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D260AD1D0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D2882C910>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D280168D0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28B8C310>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D27E99B90>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28AA2190>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28BB7790>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28D72B50>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28C65190>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28AAE890>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x0000029D28C5C7D0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'QueryAnswerResult' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_answers()\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answers:\n\u001b[1;32m---> 77\u001b[0m       results[query_id][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-semantic-answers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manswers\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     79\u001b[0m   \u001b[38;5;28mprint\u001b[39m(results[query_id])\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiracl-results.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[10], line 77\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     75\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_answers()\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answers:\n\u001b[1;32m---> 77\u001b[0m       results[query_id][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-semantic-answers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43manswer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers]\n\u001b[0;32m     79\u001b[0m   \u001b[38;5;28mprint\u001b[39m(results[query_id])\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiracl-results.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'QueryAnswerResult' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "import os\n",
    "import json\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "azure_cred = DefaultAzureCredential()\n",
    "\n",
    "search_clients = {\n",
    "  \"ja-microsoft\": SearchClient(service_endpoint, 'test-miracl-index-ja-microsoft', azure_cred),\n",
    "  \"ja-lucene\": SearchClient(service_endpoint, 'test-miracl-index-ja-lucene', azure_cred),\n",
    "  \"no-analyzer\": SearchClient(service_endpoint, 'test-miracl-index-no-analyzer', azure_cred)\n",
    "}\n",
    "\n",
    "with open('data/miracl_questions.json', 'r') as f:\n",
    "  questions = json.load(f)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for query_id, query in questions.items():\n",
    "  print(f\"Query: {query['query']}\")\n",
    "  results[query_id] = {\n",
    "    \"query\": query['query'],\n",
    "    \"positive_passages\": query['positive_passages'],\n",
    "    \"negative_passages\": query['negative_passages'],\n",
    "    \"ja-lucene-hybrid-results\": [],\n",
    "    \"ja-lucene-hybrid-answers\": [],\n",
    "    \"ja-lucene-semantic-results\": [],\n",
    "    \"ja-lucene-semantic-answers\": [],\n",
    "    \"ja-microsoft-hybrid-results\": [],\n",
    "    \"ja-microsoft-hybrid-answers\": [],\n",
    "    \"ja-microsoft-semantic-results\": [],\n",
    "    \"ja-microsoft-semantic-answers\": [],\n",
    "    \"no-analyzer-hybrid-results\": [],\n",
    "    \"no-analyzer-hybrid-answers\": [],\n",
    "    \"no-analyzer-semantic-results\": [],\n",
    "    \"no-analyzer-semantic-answers\": [],\n",
    "  }\n",
    "  for analyzer, client in search_clients.items():\n",
    "    # 1: Keyword Search (skip for now because it would be hopeless with this dataset)\n",
    "    vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=15, fields=\"embedding\", exhaustive=True)\n",
    "    # 2: Hybrid Search\n",
    "    response = await client.search(\n",
    "      search_text=query['query'],\n",
    "      vector_queries=[vector_query],\n",
    "      top=3,\n",
    "      query_language=\"ja-JP\")\n",
    "    matches = []\n",
    "    async for doc in response:\n",
    "      matches.append(doc['docid'])\n",
    "    answers = await response.get_answers()\n",
    "    if answers:\n",
    "      results[query_id][f\"{analyzer}-hybrid-answers\"] = [answer.key.replace(\"i\", \"#\") for answer in answers]\n",
    "    results[query_id][f\"{analyzer}-hybrid-results\"] = matches\n",
    "\n",
    "    # 3: Semantic Search\n",
    "    response = await client.search(\n",
    "      search_text=query['query'],\n",
    "      query_type=\"semantic\",\n",
    "      semantic_configuration_name=\"miracl-semantic\",\n",
    "      vector_queries=[\n",
    "        vector_query\n",
    "      ],\n",
    "      top=3,\n",
    "      query_answer=\"extractive\",\n",
    "      query_answer_count=3,\n",
    "      query_caption=\"extractive\",\n",
    "      query_language=\"ja-JP\")\n",
    "    matches = []\n",
    "    async for doc in response:\n",
    "      matches.append(doc['docid'])\n",
    "\n",
    "    results[query_id][f\"{analyzer}-semantic-results\"] = matches\n",
    "    answers = await response.get_answers()\n",
    "    if answers:\n",
    "      results[query_id][f\"{analyzer}-semantic-answers\"] = [answer.key.replace(\"i\", \"#\") for answer in answers]\n",
    "\n",
    "  print(results[query_id])\n",
    "\n",
    "with open('miracl-results.json', 'w') as f:\n",
    "  json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('miracl-results.json', 'r') as f:\n",
    "  results = json.load(f)\n",
    "\n",
    "for query_id, query in results.items():\n",
    "  expected = set(query['positive_passages'])\n",
    "  # TODO: Calculate NDCG@3 for each query\n",
    "  for analyzer in ['ja-lucene', 'ja-microsoft', 'no-analyzer']:\n",
    "    expected = set(query['positive_passages'])\n",
    "    actual = set(query[f\"{analyzer}-results\"])\n",
    "    recall = len(expected.intersection(actual)) / len(expected)\n",
    "\n",
    "    results[query_id][f\"{analyzer}-recall\"] = recall\n",
    "\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "df.to_csv('miracl-results.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
